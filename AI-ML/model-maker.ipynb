{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>review</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;I&#x27;m reading a lot of reviews s…</td></tr><tr><td>1</td><td>&quot;This soundtrack is my favorite…</td></tr><tr><td>1</td><td>&quot;I truly like this soundtrack a…</td></tr><tr><td>1</td><td>&quot;If you&#x27;ve played the game, you…</td></tr><tr><td>1</td><td>&quot;I am quite sure any of you act…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────┬─────────────────────────────────┐\n",
       "│ sentiment ┆ review                          │\n",
       "│ ---       ┆ ---                             │\n",
       "│ i64       ┆ str                             │\n",
       "╞═══════════╪═════════════════════════════════╡\n",
       "│ 1         ┆ I'm reading a lot of reviews s… │\n",
       "│ 1         ┆ This soundtrack is my favorite… │\n",
       "│ 1         ┆ I truly like this soundtrack a… │\n",
       "│ 1         ┆ If you've played the game, you… │\n",
       "│ 1         ┆ I am quite sure any of you act… │\n",
       "└───────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv('./data/final_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove tags and convert to lowercase\n",
    "    sentence = remove_tags(text).lower()\n",
    "\n",
    "    # remove punctuation, numbers, and single characters, and collapse multiple spaces\n",
    "    sentence = re.sub(r'[^a-z\\s]', ' ', sentence)  # remove non-alphabetic characters\n",
    "    sentence = re.sub(r'\\b[a-z]\\b', '', sentence)  # remove single characters\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()  # collapse multiple spaces\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# sentences = list(df['review'])\n",
    "\n",
    "# for sen in sentences:\n",
    "#     X.append(preprocess_text(sen))\n",
    "    \n",
    "# with open('./data/preprocessed_text.txt', 'wb') as f:\n",
    "#     pk.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/preprocessed_text.txt', 'rb') as f:\n",
    "    X = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truly like soundtrack enjoy video game music played game music enjoy truly relaxing peaceful disk one favorites scars time life death forest illusion fortress ancient dragons lost fragment drowned valley disk two draggons galdorb home chronomantique prisoners fate gale girlfriend likes zelbessdisk three best three garden god chronopolis fates jellyfish sea burning orphange dragon prayer tower stars dragon god radical dreamers unstealable jewel overall excellent soundtrack brought like video game music xander cross'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 17:23:58.522675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-27 17:23:58.600816: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-27 17:23:58.623739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-27 17:23:58.781615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-27 17:24:00.384946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenizer = Tokenizer()\n",
    "# word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "# X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/tokenizer.pkl', 'wb') as f:\n",
    "#     pk.dump(word_tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenised_data = {\n",
    "#     'X_train' : X_train,\n",
    "#     'y_train' : y_train.to_list(),\n",
    "#     'X_test' : X_test,\n",
    "#     'y_test' : y_test.to_list()\n",
    "# }\n",
    "\n",
    "# with open('./data/tokenized_data.json', 'w') as f:\n",
    "#     json.dump(tokenised_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenized_data.json', 'r') as f:\n",
    "    tokenised_data = json.load(f)\n",
    "    \n",
    "X_train = tokenised_data['X_train']\n",
    "X_test = tokenised_data['X_test']\n",
    "y_train = tokenised_data['y_train']\n",
    "y_test = tokenised_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_length = len(word_tokenizer.word_index) + 1\n",
    "# vocab_length\n",
    "vocab_length = 779656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict = dict()\n",
    "\n",
    "# with open('./data/glove.6B.100d.txt',  'r') as glove_file:\n",
    "#     for line in glove_file:\n",
    "#         records = line.split()\n",
    "#         word = records[0]\n",
    "#         vector_dimentions = np.asarray(records[1:], dtype='float32')\n",
    "#         embeddings_dict[word] = vector_dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/word_tokenizer_items.txt', 'w') as f:\n",
    "#     pk.dump(word_tokenizer.word_index.items(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((vocab_length, 100))\n",
    "# for word, index in word_tokenizer.word_index.items():\n",
    "#     embedding_vector = embeddings_dict.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(embedding_matrix), embedding_matrix[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/embedding_matrix.txt', 'wb') as f:\n",
    "#     pk.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/embedding_matrix.txt', 'rb') as f:\n",
    "    embedding_matrix = pk.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dropout, GlobalMaxPooling1D, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730030071.343461    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.555182    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.555417    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1730030071.566749    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.567081    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.567248    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.649085    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.649288    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1730030071.649473    7569 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-27 17:24:31.650064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-10-27 17:24:32.880479: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 311862400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 17:24:36.539471: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1279999200 exceeds 10% of free system memory.\n",
      "2024-10-27 17:24:38.029839: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25599984 exceeds 10% of free system memory.\n",
      "2024-10-27 17:24:38.146850: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25599984 exceeds 10% of free system memory.\n",
      "2024-10-27 17:24:38.160460: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 25599984 exceeds 10% of free system memory.\n",
      "2024-10-27 17:24:40.948756: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 14ms/step - acc: 0.8495 - loss: 0.3417 - val_acc: 0.8874 - val_loss: 0.2929\n",
      "Epoch 2/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 15ms/step - acc: 0.8983 - loss: 0.2480 - val_acc: 0.9099 - val_loss: 0.2506\n",
      "Epoch 3/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 15ms/step - acc: 0.9072 - loss: 0.2298 - val_acc: 0.9152 - val_loss: 0.2262\n",
      "Epoch 4/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 15ms/step - acc: 0.9113 - loss: 0.2204 - val_acc: 0.9184 - val_loss: 0.2357\n",
      "Epoch 5/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 15ms/step - acc: 0.9143 - loss: 0.2141 - val_acc: 0.9193 - val_loss: 0.2281\n",
      "Epoch 6/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 15ms/step - acc: 0.9164 - loss: 0.2091 - val_acc: 0.9224 - val_loss: 0.2109\n",
      "Epoch 7/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 15ms/step - acc: 0.9184 - loss: 0.2049 - val_acc: 0.9247 - val_loss: 0.2142\n",
      "Epoch 8/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m457s\u001b[0m 18ms/step - acc: 0.9199 - loss: 0.2015 - val_acc: 0.9255 - val_loss: 0.2185\n",
      "Epoch 9/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 18ms/step - acc: 0.9213 - loss: 0.1988 - val_acc: 0.9263 - val_loss: 0.2061\n",
      "Epoch 10/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m455s\u001b[0m 18ms/step - acc: 0.9222 - loss: 0.1965 - val_acc: 0.9276 - val_loss: 0.2090\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):  # Force the model to run on the GPU\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),  # Keep the output 3D\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        GlobalMaxPooling1D(),  # Now works as it has a 3D input\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model (ensure loss is compatible with mixed precision)\n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    lstm_model_history = lstm_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 4ms/step - acc: 0.9113 - loss: 0.2338\n",
      "Test Score : 0.23351474106311798\n",
      "Test Accuracy: 0.9113749861717224\n"
     ]
    }
   ],
   "source": [
    "score = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Score : {score[0]}\\nTest Accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "save_model(lstm_model, './models/lstm_model_new.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_enhanced.save('./models/lstm_model_enhanced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "askhdbaksdh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./models/lstm_model_new.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(text, tokenizer, model):\n",
    "    clean_text = preprocess_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([clean_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=100)\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    prediction = 'Negative' if prediction< 0.5 else 'Positive'\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "text = \"This DVD will be a disappointment if you get it hoping to see some substantial portion of the acts of the various comics listed on the cover. All you get here are snippets of performance, at best. The rest is just loose-leaf reminiscence about the good old days in Boston, in the early 80's, when a lot of comics were hanging out together and getting their start.It's like a frat house reunion. There's a lot of lame nostalgia. There are quite a few guffaws recalling jokes (practical and otherwise)perpetrated - back then. But you had to have been there to appreciate all the basically good ol' boy camaraderie. If you weren't actually a part of that scene, all this joshing and jostling will fall flat.If you want to actually hear some of these comics' routines - you will have to look elsewhere.\"\n",
    "\n",
    "pred = model_predict(text=text, tokenizer=tokenizer, model=model)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
