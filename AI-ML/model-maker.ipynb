{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>sentiment</th><th>review</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;I&#x27;m reading a lot of reviews s…</td></tr><tr><td>1</td><td>&quot;This soundtrack is my favorite…</td></tr><tr><td>1</td><td>&quot;I truly like this soundtrack a…</td></tr><tr><td>1</td><td>&quot;If you&#x27;ve played the game, you…</td></tr><tr><td>1</td><td>&quot;I am quite sure any of you act…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────┬─────────────────────────────────┐\n",
       "│ sentiment ┆ review                          │\n",
       "│ ---       ┆ ---                             │\n",
       "│ i64       ┆ str                             │\n",
       "╞═══════════╪═════════════════════════════════╡\n",
       "│ 1         ┆ I'm reading a lot of reviews s… │\n",
       "│ 1         ┆ This soundtrack is my favorite… │\n",
       "│ 1         ┆ I truly like this soundtrack a… │\n",
       "│ 1         ┆ If you've played the game, you… │\n",
       "│ 1         ┆ I am quite sure any of you act… │\n",
       "└───────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv('./data/final_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove tags and convert to lowercase\n",
    "    sentence = remove_tags(text).lower()\n",
    "\n",
    "    # remove punctuation, numbers, and single characters, and collapse multiple spaces\n",
    "    sentence = re.sub(r'[^a-z\\s]', ' ', sentence)  # remove non-alphabetic characters\n",
    "    sentence = re.sub(r'\\b[a-z]\\b', '', sentence)  # remove single characters\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()  # collapse multiple spaces\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentence = ' '.join([word for word in sentence.split() if word not in stop_words])\n",
    "    \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# sentences = list(df['review'])\n",
    "\n",
    "# for sen in sentences:\n",
    "#     X.append(preprocess_text(sen))\n",
    "    \n",
    "# with open('./data/preprocessed_text.txt', 'wb') as f:\n",
    "#     pk.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/preprocessed_text.txt', 'rb') as f:\n",
    "    X = pk.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truly like soundtrack enjoy video game music played game music enjoy truly relaxing peaceful disk one favorites scars time life death forest illusion fortress ancient dragons lost fragment drowned valley disk two draggons galdorb home chronomantique prisoners fate gale girlfriend likes zelbessdisk three best three garden god chronopolis fates jellyfish sea burning orphange dragon prayer tower stars dragon god radical dreamers unstealable jewel overall excellent soundtrack brought like video game music xander cross'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:14:59.656353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-16 11:14:59.667810: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-16 11:14:59.680076: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-16 11:14:59.708077: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-16 11:15:01.247149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
    "X_test = word_tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenised_data = {\n",
    "#     'X_train' : X_train,\n",
    "#     'y_train' : y_train.to_list(),\n",
    "#     'X_test' : X_test,\n",
    "#     'y_test' : y_test.to_list()\n",
    "# }\n",
    "\n",
    "# with open('./data/tokenized_data.json', 'w') as f:\n",
    "#     json.dump(tokenised_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/tokenized_data.json', 'r') as f:\n",
    "#     tokenised_data = json.load(f)\n",
    "    \n",
    "# X_train = tokenised_data['X_train']\n",
    "# X_test = tokenised_data['X_test']\n",
    "# y_train = tokenised_data['y_train']\n",
    "# y_test = tokenised_data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_length = len(word_tokenizer.word_index) + 1\n",
    "# vocab_length\n",
    "vocab_length = 779656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = dict()\n",
    "\n",
    "with open('./data/glove.6B.100d.txt',  'r') as glove_file:\n",
    "    for line in glove_file:\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimentions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dict[word] = vector_dimentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/word_tokenizer_items.txt', 'w') as f:\n",
    "#     pk.dump(word_tokenizer.word_index.items(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray,\n",
       " array([-1.97439998e-01,  4.48309988e-01,  1.36889994e-01, -1.55949995e-01,\n",
       "         9.35999990e-01,  7.29860008e-01,  3.40990007e-01, -3.38959992e-01,\n",
       "        -8.95690024e-02, -4.77059990e-01,  3.51119995e-01, -4.21979994e-01,\n",
       "        -1.22210003e-01, -6.33750036e-02, -4.58200008e-01,  7.87230015e-01,\n",
       "         9.40450013e-01,  8.11010003e-02, -2.32240006e-01,  4.07779992e-01,\n",
       "         3.32580000e-01, -4.44579989e-01, -4.71170008e-01,  1.48519993e-01,\n",
       "         9.63079989e-01, -6.52669966e-02, -5.36610000e-02, -6.74740016e-01,\n",
       "        -4.23640013e-01,  9.43920016e-02, -3.86680007e-01,  1.82370007e-01,\n",
       "        -1.28460005e-01, -2.19520003e-01, -5.89929998e-01,  7.36020029e-01,\n",
       "        -2.40089998e-01,  3.23920012e-01, -2.46629998e-01, -4.06839997e-01,\n",
       "        -5.24680018e-01,  4.61739987e-01, -1.49360001e-01, -1.19989999e-01,\n",
       "        -1.39899999e-01, -4.49440002e-01, -2.65650004e-01, -7.00609982e-01,\n",
       "         3.01880002e-01, -1.12089999e-01,  6.63230002e-01,  3.96979988e-01,\n",
       "         6.91579998e-01,  8.34420025e-01, -5.27170002e-01, -2.53139997e+00,\n",
       "         1.32809997e-01,  3.02529991e-01,  1.10619998e+00,  7.22210016e-03,\n",
       "         2.60309994e-01,  1.15840006e+00, -7.93299973e-02, -7.66589999e-01,\n",
       "         1.26230001e+00, -6.20710015e-01,  5.98209977e-01,  7.35390007e-01,\n",
       "         3.85729998e-01, -4.02929991e-01, -3.14400010e-02,  7.78630018e-01,\n",
       "         3.15250009e-01,  1.90029994e-01, -6.58209980e-01,  4.05479997e-01,\n",
       "         5.35959983e-03,  5.52739985e-02, -1.22379994e+00, -4.89119999e-02,\n",
       "        -3.05110008e-01,  4.44730014e-01, -3.38259995e-01, -2.21330002e-01,\n",
       "        -1.32140005e+00, -6.47610009e-01, -4.40210015e-01, -1.49100006e+00,\n",
       "        -2.24949997e-02,  6.03459999e-02,  1.48330003e-01,  4.41619992e-01,\n",
       "         7.97869980e-01, -2.80759990e-01, -2.94000003e-02, -1.56560004e-01,\n",
       "        -1.26499996e-01, -5.69679976e-01,  1.53739995e-03,  6.66000009e-01]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_matrix), embedding_matrix[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/embedding_matrix.txt', 'wb') as f:\n",
    "#     pk.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/embedding_matrix.txt', 'rb') as f:\n",
    "#     emb_mat = pk.load(f)\n",
    "    \n",
    "# type(emb_mat), emb_mat[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GRU, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(hp):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_length,\n",
    "                  100,\n",
    "                  weights=[embedding_matrix],\n",
    "                  input_length=maxlen,\n",
    "                  trainable=False),\n",
    "        \n",
    "        LSTM(hp.Int('lstm_units', min_value=64, max_value=256, step=32)),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from ./tuning_dir/lstm_tuning/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726465675.300605    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.465477    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.465733    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.468920    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.469205    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.469408    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.548135    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.548336    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1726465675.548501    7782 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-16 11:17:55.549186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4232 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    build_lstm_model,            \n",
    "    objective='val_accuracy',    \n",
    "    max_trials=10,               \n",
    "    executions_per_trial=1,      \n",
    "    directory='./tuning_dir',   \n",
    "    project_name='lstm_tuning'   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.search(X_train_, y_train_, \n",
    "#              epochs=5, \n",
    "#              validation_data=(X_val, y_val),\n",
    "#              batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:18:02.166118: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 311862400 exceeds 10% of free system memory.\n",
      "2024-09-16 11:18:03.200547: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 311862400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">77,965,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">225,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │    \u001b[38;5;34m77,965,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)            │       \u001b[38;5;34m225,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m193\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,190,817</span> (298.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,190,817\u001b[0m (298.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,217</span> (879.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,217\u001b[0m (879.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,965,600</span> (297.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m77,965,600\u001b[0m (297.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:22:13.503271: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 311862400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 11:22:14.150119: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1279999200 exceeds 10% of free system memory.\n",
      "2024-09-16 11:22:17.585002: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 15ms/step - acc: 0.8481 - loss: 0.3446 - val_acc: 0.8737 - val_loss: 0.3097\n",
      "Epoch 2/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 15ms/step - acc: 0.8982 - loss: 0.2481 - val_acc: 0.9092 - val_loss: 0.2614\n",
      "Epoch 3/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 15ms/step - acc: 0.9068 - loss: 0.2302 - val_acc: 0.9127 - val_loss: 0.2451\n",
      "Epoch 4/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 15ms/step - acc: 0.9112 - loss: 0.2210 - val_acc: 0.9176 - val_loss: 0.2214\n",
      "Epoch 5/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 15ms/step - acc: 0.9139 - loss: 0.2151 - val_acc: 0.9197 - val_loss: 0.2238\n",
      "Epoch 6/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 15ms/step - acc: 0.9166 - loss: 0.2088 - val_acc: 0.9193 - val_loss: 0.2287\n",
      "Epoch 7/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 15ms/step - acc: 0.9182 - loss: 0.2053 - val_acc: 0.9226 - val_loss: 0.2171\n",
      "Epoch 8/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 15ms/step - acc: 0.9196 - loss: 0.2020 - val_acc: 0.9243 - val_loss: 0.2176\n",
      "Epoch 9/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 15ms/step - acc: 0.9207 - loss: 0.1998 - val_acc: 0.9259 - val_loss: 0.2173\n",
      "Epoch 10/10\n",
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 15ms/step - acc: 0.9219 - loss: 0.1972 - val_acc: 0.9271 - val_loss: 0.2189\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):  # Force the model to run on the GPU\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        LSTM(128, return_sequences=True),  # Keep the output 3D\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        GlobalMaxPooling1D(),  # Now works as it has a 3D input\n",
    "        \n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model (ensure loss is compatible with mixed precision)\n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    lstm_model_history = lstm_model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=128,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        validation_data=(X_val, y_val)https://colab.research.google.com/drive/1xpAF-8nGvsbqXX4DvgmOPczaXASdMRVY?authuser=2#scrollTo=E09V-awqVow2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss='binary_crossentropy',\n",
    "#     metrics=['acc']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 19:21:23.520660: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 320000000 exceeds 10% of free system memory.\n",
      "2024-09-15 19:21:24.454370: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2ms/step - accuracy: 0.9044 - loss: 0.2341\n",
      "Test Score : 0.23308660089969635\n",
      "Test Accuracy: 0.9048100113868713\n"
     ]
    }
   ],
   "source": [
    "score = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Score : {score[0]}\\nTest Accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25000/25000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 4ms/step - acc: 0.9098 - loss: 0.2433\n",
      "Test Score : 0.24265900254249573\n",
      "Test Accuracy: 0.910147488117218\n"
     ]
    }
   ],
   "source": [
    "score = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f'Test Score : {score[0]}\\nTest Accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "lstm_model.save('./models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2024-09-15 20:44:59.829545: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 311862400 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# best_model.save('./models/Best_LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflowjs\n",
      "  Downloading tensorflowjs-4.21.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting flax>=0.7.2 (from tensorflowjs)\n",
      "  Downloading flax-0.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting importlib_resources>=5.9.0 (from tensorflowjs)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jax>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting jaxlib>=0.4.13 (from tensorflowjs)\n",
      "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
      "Requirement already satisfied: tensorflow<3,>=2.13.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflowjs) (2.17.0)\n",
      "Collecting tf-keras>=2.13.0 (from tensorflowjs)\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
      "  Downloading tensorflow_decision_forests-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: six<2,>=1.16.0 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from tensorflowjs) (1.16.0)\n",
      "Collecting tensorflow-hub>=0.16.1 (from tensorflowjs)\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting packaging~=23.1 (from tensorflowjs)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting msgpack (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting optax (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting orbax-checkpoint (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading orbax_checkpoint-0.6.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorstore (from flax>=0.7.2->tensorflowjs)\n",
      "  Downloading tensorstore-0.1.65-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1 in /home/shanks/.local/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (13.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in /home/shanks/.local/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (4.10.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/shanks/.local/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/shanks/.local/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /home/shanks/.local/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/shanks/.local/lib/python3.10/site-packages (from jax>=0.4.13->tensorflowjs) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (72.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.65.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.1)\n",
      "Requirement already satisfied: pandas in /home/shanks/.local/lib/python3.10/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
      "Requirement already satisfied: wheel in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.43.0)\n",
      "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Downloading wurlitzer-3.1.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting ydf (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
      "  Downloading ydf-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: namex in /home/shanks/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/shanks/.local/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/shanks/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/shanks/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/shanks/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/shanks/.local/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/shanks/.local/lib/python3.10/site-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.18.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/shanks/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/shanks/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/shanks/.local/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.3)\n",
      "Collecting chex>=0.1.86 (from optax->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading chex-0.1.86-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting etils[epy] (from optax->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading etils-1.9.4-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nest_asyncio in /home/shanks/.local/lib/python3.10/site-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
      "Collecting humanize (from orbax-checkpoint->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading humanize-4.10.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/shanks/.local/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/shanks/.local/lib/python3.10/site-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2024.1)\n",
      "Collecting toolz>=0.9.0 (from chex>=0.1.86->optax->flax>=0.7.2->tensorflowjs)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/shanks/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/shanks/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.5)\n",
      "Requirement already satisfied: fsspec in /home/shanks/.local/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2024.6.0)\n",
      "Requirement already satisfied: zipp in /home/shanks/anaconda3/envs/pytorch/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.17.0)\n",
      "Downloading tensorflowjs-4.21.0-py3-none-any.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flax-0.9.0-py3-none-any.whl (780 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.7/780.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading jax-0.4.31-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl (88.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_decision_forests-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optax-0.2.3-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orbax_checkpoint-0.6.3-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.7/269.7 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorstore-0.1.65-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading wurlitzer-3.1.1-py3-none-any.whl (8.6 kB)\n",
      "Downloading ydf-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading chex-0.1.86-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanize-4.10.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.9.4-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.3/164.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ydf, wurlitzer, toolz, packaging, msgpack, importlib_resources, humanize, etils, tensorstore, jaxlib, jax, orbax-checkpoint, chex, tf-keras, optax, tensorflow-hub, tensorflow-decision-forests, flax, tensorflowjs\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "roboflow 1.1.37 requires chardet==4.0.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed chex-0.1.86 etils-1.9.4 flax-0.9.0 humanize-4.10.0 importlib_resources-6.4.5 jax-0.4.31 jaxlib-0.4.31 msgpack-1.1.0 optax-0.2.3 orbax-checkpoint-0.6.3 packaging-23.2 tensorflow-decision-forests-1.10.0 tensorflow-hub-0.16.1 tensorflowjs-4.21.0 tensorstore-0.1.65 tf-keras-2.17.0 toolz-0.12.1 wurlitzer-3.1.1 ydf-0.7.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflowjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-15 20:47:22.793449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-15 20:47:22.879356: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-15 20:47:22.902584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-15 20:47:23.061961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-15 20:47:24.061751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "failed to lookup keras version from the file,\n",
      "    this is likely a weight only file\n"
     ]
    }
   ],
   "source": [
    "# !tensorflowjs_converter --input_format keras ./models/Best_LSTM_model.h5 ./models/tf_js_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
